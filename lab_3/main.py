# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j_lTqLoznfgRJI8E0O2IvByfZP2tkCo1

### Я решил поменять датасет
___

### Библиотеки
___
"""

import numpy as np
import pandas as pd
import holidays
from datetime import timedelta
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import TargetEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from catboost import CatBoostRegressor
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import tqdm

# Установим общий seed для воспроизводимости
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

"""### Анализ и подготовка данных
___
"""

data = pd.read_csv("lab_3/retail_store_inventory.csv")
print("Исходный датасет:")
print(data.tail())

"""### Беру продажи в одном из регионов и убираю ненужные столбцы

Беру East, выбрасываю ненужные колонки и добавляю признак - выше ли цена чем у конкурента (bool)
"""

data = data[data["Region"] == "East"].copy()
data["Price_Higher_Than_Competitor"] = (
    data["Price"] > data["Competitor Pricing"]
).astype(int)

# ВЕРНУЛИ 'Seasonality', так как это полезный признак!
data = data.drop(
    columns=[
        "Store ID",
        "Region",
        "Inventory Level",
        "Units Ordered",  # Скорее всего, утечка данных, оставляем удаленным
        "Demand Forecast",  # Скорее всего, утечка данных, оставляем удаленным
        "Price",
        "Competitor Pricing",
        "Holiday/Promotion",
    ]
)
data["Date"] = pd.to_datetime(data["Date"])
print("\nДатасет после выбора региона и создания признака:")
print(data.sample(5))

"""### Создание признаков, связанных с датами (КРИТИЧЕСКИ ВАЖНОЕ ИСПРАВЛЕНИЕ)"""

# 1. АВТОМАТИЧЕСКИ ОПРЕДЕЛЯЕМ ГОДЫ В ДАТАСЕТЕ
min_year = data["Date"].dt.year.min()
max_year = data["Date"].dt.year.max()
all_years = range(min_year, max_year + 1)

# календарь праздников для ВСЕХ лет
country_holidays = holidays.US(years=all_years)
# Преобразуем в set для быстрого поиска
holiday_dates = set(country_holidays.keys())

data["Weekday"] = data["Date"].dt.weekday
data["Month"] = data["Date"].dt.month
data["Weekend"] = data["Date"].dt.dayofweek.isin([5, 6]).astype(int)

# Использование .dt.date для сравнения с объектами datetime.date в holiday_dates
data["Holiday"] = data["Date"].dt.date.isin(holiday_dates).astype(int)

# Признак "Завтра выходной или праздник"
tomorrow_dates = data["Date"] + pd.Timedelta(days=1)
data["Tomorrow_Weekend_or_Holiday"] = tomorrow_dates.apply(
    lambda x: int(x.date() in holiday_dates or x.dayofweek in [5, 6])
)


# 2. ВЕКТОРИЗАЦИЯ ФУНКЦИИ holiday_within_7_days для скорости
def get_next_7_days(dates):
    """Генерирует все даты в течение следующих 7 дней для заданного набора дат."""
    all_future_dates = set()
    for date in dates:
        for d in range(1, 8):
            all_future_dates.add((date + timedelta(days=d)).date())
    return all_future_dates

# Создаем набор всех дат, которые попадают в 7-дневный интервал перед праздником
future_dates_to_check = get_next_7_days(data["Date"].dt.to_pydatetime())
data["Holiday_Within_7_Days"] = data["Date"].dt.date.apply(
    lambda x: int(x in future_dates_to_check)
)

print("\nДатасет с новыми признаками:")
print(data.sample(5))

"""### Визуализация"""

sns.histplot(data["Units Sold"], bins=50, kde=True)
plt.title("Распределение продаж (Units Sold)")
plt.xlabel("Units Sold")
plt.ylabel("Количество")
plt.show()
# Примечание: распределение смещено, поэтому будем использовать log-преобразование!

sns.boxplot(x="Category", y="Units Sold", data=data)
plt.title("Продажи по категориям")
plt.show()

sns.barplot(x="Weekday", y="Units Sold", data=data, ci=None)
plt.title("Средние продажи по дням недели")
plt.show()

sns.barplot(x="Month", y="Units Sold", data=data, ci=None)
plt.title("Средние продажи по месяцу")
plt.show()

sns.barplot(x="Weekend", y="Units Sold", data=data, ci=None)
plt.title("Продажи в выходные/будни")
plt.show()

sns.barplot(x="Holiday", y="Units Sold", data=data)
plt.title("Продажи в праздники/не праздники")
plt.show()

numeric_cols_for_corr = data.select_dtypes(include="number").columns
corr = data[numeric_cols_for_corr].corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Корреляция числовых признаков")
plt.show()

sns.boxplot(x="Weather Condition", y="Units Sold", data=data)
plt.title("Продажи по погодным условиям")
plt.show()

"""### Заполнение пропусков"""

n_missing = 100
missing_indices = np.random.choice(data.index, size=n_missing, replace=False)
data.loc[missing_indices, "Units Sold"] = np.nan

group_cols = [
    "Product ID",
    "Price_Higher_Than_Competitor",
    "Weekend",
    "Holiday",
    "Tomorrow_Weekend_or_Holiday",
    "Holiday_Within_7_Days",
    "Month",
]

median_per_group = data.groupby(group_cols)["Units Sold"].median().dropna()
mask_nan = data["Units Sold"].isna()

for idx in data[mask_nan].index:
    key = tuple(data.loc[idx, group_cols])
    if key in median_per_group.index:
        data.at[idx, "Units Sold"] = median_per_group.loc[key]

# Удаляем оставшиеся nan
data = data.dropna(subset=["Units Sold"])
print(f"\nКоличество оставшихся NaN после удаления: {data['Units Sold'].isna().sum()}")

"""### Создание наборов данных"""

# Признаки для обеих моделей
features = [
    "Product ID",
    "Category",
    "Discount",
    "Weather Condition",
    "Seasonality", # ВЕРНУЛИ
    "Price_Higher_Than_Competitor",
    "Weekday",
    "Month",
    "Weekend",
    "Holiday",
    "Tomorrow_Weekend_or_Holiday",
    "Holiday_Within_7_Days",
]

# Целевая переменная с log-преобразованием
y = np.log1p(data["Units Sold"])
x = data[features]

# Разделение на train/test для всех моделей
x_train_raw, x_test_raw, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=SEED
)

"""### Подготовка данных для CatBoost (Сырые категориальные признаки)"""

# CatBoost использует сырые данные. Копируем для чистоты эксперимента.
x_cat_train = x_train_raw.copy()
x_cat_test = x_test_raw.copy()

# Определяем категориальные столбцы для CatBoost (ОСТАВЛЯЕМ ИХ СТРОКАМИ)
cat_cols = ["Product ID", "Category", "Weather Condition", "Seasonality"]

# Преобразуем числовые категориальные признаки в строки для CatBoost
for col in cat_cols:
    x_cat_train[col] = x_cat_train[col].astype(str)
    x_cat_test[col] = x_cat_test[col].astype(str)

"""### Подготовка данных для PyTorch (Target Encoding + Scaling)"""

# PyTorch (FFNN) требует числовые признаки.
x_torch_train = x_train_raw.copy()
x_torch_test = x_test_raw.copy()

# Target Encoding для PyTorch
te_product = TargetEncoder(random_state=SEED)
x_torch_train["Product ID"] = te_product.fit_transform(x_torch_train[["Product ID"]], y_train)
x_torch_test["Product ID"] = te_product.transform(x_torch_test[["Product ID"]])

te_category = TargetEncoder(random_state=SEED)
x_torch_train["Category"] = te_category.fit_transform(x_torch_train[["Category"]], y_train)
x_torch_test["Category"] = te_category.transform(x_torch_test[["Category"]])

te_weather = TargetEncoder(random_state=SEED)
x_torch_train["Weather Condition"] = te_weather.fit_transform(
    x_torch_train[["Weather Condition"]], y_train
)
x_torch_test["Weather Condition"] = te_weather.transform(x_torch_test[["Weather Condition"]])

te_seasonality = TargetEncoder(random_state=SEED)
x_torch_train["Seasonality"] = te_seasonality.fit_transform(
    x_torch_train[["Seasonality"]], y_train
)
x_torch_test["Seasonality"] = te_seasonality.transform(x_torch_test[["Seasonality"]])


# Scaling для PyTorch
numeric_cols_for_nn = x_torch_train.select_dtypes(include="number").columns
scaler = StandardScaler()

x_torch_train[numeric_cols_for_nn] = scaler.fit_transform(x_torch_train[numeric_cols_for_nn])
x_torch_test[numeric_cols_for_nn] = scaler.transform(x_torch_test[numeric_cols_for_nn])

"""### Градиентный бустинг (CatBoost)"""

# 1. Определяем индексы категориальных признаков для CatBoost
cat_features_indices = [x_cat_train.columns.get_loc(col) for col in cat_cols]

cat_model = CatBoostRegressor(
    iterations=1000,
    depth=4,
    l2_leaf_reg=100,
    loss_function="RMSE",
    eval_metric="RMSE",
    verbose=100,
    random_state=SEED, # Используем общий SEED
    cat_features=cat_features_indices # ПЕРЕДАЕМ СЫРЫЕ КАТЕГОРИАЛЬНЫЕ ПРИЗНАКИ
)

print("Начало обучения CatBoost...")
cat_model.fit(
    x_cat_train,
    y_train,
    eval_set=(x_cat_test, y_test),
    early_stopping_rounds=50,
    use_best_model=True
)

y_pred_log = cat_model.predict(x_cat_test)

# ОБРАТНОЕ ПРЕОБРАЗОВАНИЕ ДЛЯ МЕТРИК
y_pred = np.expm1(y_pred_log)
y_true = np.expm1(y_test)

rmse = np.sqrt(mean_squared_error(y_true, y_pred))
r2 = r2_score(y_true, y_pred)

print(f"\n--- CatBoost Results (After Inverse Transform) ---")
print(f"R2 Score: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

"""### Полносвязка (PyTorch)"""

def train_model(
    model,
    train_loader,
    val_loader,
    dataset_sizes,
    criterion,
    optimizer,
    scheduler,
    num_epochs=50,
):
    best_model_wts = model.state_dict()
    best_rmse = float("inf")

    losses = {"train": [], "val": []}
    metrics = {"train": {"RMSE": [], "R2": []}, "val": {"RMSE": [], "R2": []}}

    for epoch in range(num_epochs):
        for phase in ["train", "val"]:
            if phase == "train":
                model.train()
                loader = train_loader
            else:
                model.eval()
                loader = val_loader

            running_loss = 0.0
            all_labels_log = []
            all_preds_log = []

            for inputs, labels in tqdm.tqdm(loader, leave=False, desc=f"{phase} iter:"):
                # inputs, labels = inputs.to(device), labels.to(device) # Уже на GPU, если доступно

                if phase == "train":
                    optimizer.zero_grad()

                with torch.set_grad_enabled(phase == "train"):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels) # Loss считается на log-шкале

                    if phase == "train":
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                all_labels_log.append(labels.detach().cpu().numpy())
                all_preds_log.append(outputs.detach().cpu().numpy())

            epoch_loss = running_loss / dataset_sizes[phase]

            # Метрики считаем на исходной шкале (Units Sold)
            y_true_log = np.vstack(all_labels_log)
            y_pred_log = np.vstack(all_preds_log)

            # ОБРАТНОЕ ПРЕОБРАЗОВАНИЕ
            y_true = np.expm1(y_true_log)
            y_pred = np.expm1(y_pred_log)

            # Использование клиппинга, чтобы избежать ошибки при R2, если log1p(0) не используется
            y_true[y_true < 0] = 0
            y_pred[y_pred < 0] = 0

            epoch_rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            epoch_r2 = r2_score(y_true, y_pred)

            losses[phase].append(epoch_loss)
            metrics[phase]["RMSE"].append(epoch_rmse)
            metrics[phase]["R2"].append(epoch_r2)

            tqdm.tqdm.write(
                f"{phase} Epoch {epoch + 1}/{num_epochs} "
                f"Loss (Log): {epoch_loss:.4f} RMSE (Actual): {epoch_rmse:.4f} R2 (Actual): {epoch_r2:.4f}"
            )

            # сохраняем лучшую модель по RMSE на валидации
            if phase == "val" and epoch_rmse < best_rmse:
                best_rmse = epoch_rmse
                best_model_wts = model.state_dict()

        # шаг scheduler по RMSE на валидации
        scheduler.step(epoch_rmse)

    print(f"Best val RMSE: {best_rmse:.4f}")
    model.load_state_dict(best_model_wts)
    return model, (losses, metrics)

def plot_grafs(metrics):
    losses, metric_vals = metrics

    # Plot Loss (Log Scale)
    plt.figure(figsize=(16, 6))
    plt.plot(losses["train"], label="Train Loss (Log Scale)")
    plt.plot(losses["val"], label="Validation Loss (Log Scale)")
    plt.title("Training and Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

    # Plot RMSE (Actual Scale)
    plt.figure(figsize=(16, 6))
    plt.plot(metric_vals["train"]["RMSE"], label="Train RMSE (Actual Scale)")
    plt.plot(metric_vals["val"]["RMSE"], label="Validation RMSE (Actual Scale)")
    plt.title("Training and Validation RMSE")
    plt.xlabel("Epochs")
    plt.ylabel("RMSE")
    plt.legend()
    plt.show()

    # Plot R2 (Actual Scale)
    plt.figure(figsize=(16, 6))
    plt.plot(metric_vals["train"]["R2"], label="Train R2 (Actual Scale)")
    plt.plot(metric_vals["val"]["R2"], label="Validation R2 (Actual Scale)")
    plt.title("Training and Validation R2 Score")
    plt.xlabel("Epochs")
    plt.ylabel("R2")
    plt.legend()
    plt.show()

class Model(nn.Module):
    def __init__(self, input_dim, dropout=0.2):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, 1)  # выходной слой для регрессии
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.relu(self.fc3(x))
        x = self.dropout(x)
        x = self.fc4(x)
        return x

# Передача предобработанных данных PyTorch
X_train_tensor = torch.tensor(x_torch_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(x_torch_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Перенос данных на устройство (если нужно, но Dataloader может это сделать позже)
X_train_tensor = X_train_tensor.to(device)
y_train_tensor = y_train_tensor.to(device)
X_test_tensor = X_test_tensor.to(device)
y_test_tensor = y_test_tensor.to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

dataset_sizes = {"train": len(train_dataset), "val": len(test_dataset)}

model = Model(input_dim=X_train_tensor.shape[1]).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", factor=0.5, patience=5, verbose=True
)

print("\nНачало обучения PyTorch FFNN...")
model_trained, metrics = train_model(
    model,
    train_loader,
    val_loader,
    dataset_sizes,
    criterion,
    optimizer,
    scheduler,
    num_epochs=50,
)

plot_grafs(metrics)
